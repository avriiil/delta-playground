{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e98271-77e5-4bf4-9862-330c1754b5d9",
   "metadata": {},
   "source": [
    "# Delta Lake for ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150f6740-797e-4b0f-9cd2-e2d29f6528b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from delta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7a07aa0-a7d7-4827-8a9d-bb2998c36bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = SparkSession.builder.master(\"local[4]\").appName(\"parallel\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d7095dc-50ce-4afa-ae81-bfd9e082187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Delta table path\n",
    "delta_table_path = \"/data/github_data\"\n",
    "\n",
    "# Step 1: Fetch data from GitHub API\n",
    "def fetch_github_data():\n",
    "    # GitHub API URL\n",
    "    repo = \"apache/spark\"\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=1)\n",
    "    params = {\n",
    "        \"state\": \"all\",\n",
    "        \"since\": start_date.isoformat(),\n",
    "        \"per_page\": 100\n",
    "    }\n",
    "    \n",
    "    issues_url = f\"https://api.github.com/repos/{repo}/issues\"\n",
    "    prs_url = f\"https://api.github.com/repos/{repo}/pulls\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github.v3+json\"\n",
    "    }\n",
    "    \n",
    "    issues_response = requests.get(issues_url, headers=headers, params=params)\n",
    "    prs_response = requests.get(prs_url, headers=headers, params=params)\n",
    "    \n",
    "    issues = issues_response.json()\n",
    "    prs = prs_response.json()\n",
    "    \n",
    "    return issues, prs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e87a8128-47f8-4e3f-b5f5-0cbef571baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "issues, prs = fetch_github_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fab21e6b-94fc-4ea0-a64b-f9f893970265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 100)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(issues), len(prs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4457fb2-26f1-4c8f-bb81-7582b9b420b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4bd51-7365-4b1e-9860-d749d3146773",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "916f35e7-0c15-4bcd-8d99-b048421eb457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Transform data\n",
    "def transform_data(issues, prs):\n",
    "    issues_df = spark.createDataFrame(issues)\n",
    "    prs_df = spark.createDataFrame(prs)\n",
    "    \n",
    "    # Add a new column to differentiate between issues and PRs\n",
    "    issues_df = issues_df.withColumn(\"type\", lit(\"issue\"))\n",
    "    prs_df = prs_df.withColumn(\"type\", lit(\"pull_request\"))\n",
    "    \n",
    "    # Union the dataframes\n",
    "    df = issues_df.union(prs_df)\n",
    "    \n",
    "    # Basic data cleaning: remove duplicates\n",
    "    df = df.dropDuplicates()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13e72f15-64a7-44a8-9107-969137447d9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkValueError",
     "evalue": "[CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkValueError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43missues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mtransform_data\u001b[0;34m(issues, prs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_data\u001b[39m(issues, prs):\n\u001b[0;32m----> 3\u001b[0m     issues_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43missues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     prs_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(prs)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Add a new column to differentiate between issues and PRs\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pyspark-350-delta-320/lib/python3.11/site-packages/pyspark/sql/session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[1;32m   1442\u001b[0m     )\n\u001b[0;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pyspark-350-delta-320/lib/python3.11/site-packages/pyspark/sql/session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[0;32m~/miniforge3/envs/pyspark-350-delta-320/lib/python3.11/site-packages/pyspark/sql/session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m-> 1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1094\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[1;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[0;32m~/miniforge3/envs/pyspark-350-delta-320/lib/python3.11/site-packages/pyspark/sql/session.py:969\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    955\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[1;32m    956\u001b[0m     _merge_type,\n\u001b[1;32m    957\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    966\u001b[0m     ),\n\u001b[1;32m    967\u001b[0m )\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m--> 969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    972\u001b[0m     )\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m schema\n",
      "\u001b[0;31mPySparkValueError\u001b[0m: [CANNOT_DETERMINE_TYPE] Some of types cannot be determined after inferring."
     ]
    }
   ],
   "source": [
    "df = transform_data(issues, prs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5f42e2-8d5e-4fb2-a4a4-eadc5222cc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f828c98-6273-4529-9b0d-f1b4e9ed8b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c037d-6bce-40b8-bd18-2d04b06f04ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72410e0d-2b35-4cbf-a3ad-ee4c2d269ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load data to Delta Lake\n",
    "def load_data_to_delta(df):\n",
    "    # Check if the Delta table exists\n",
    "    if not DeltaTable.isDeltaTable(spark, delta_table_path):\n",
    "        # If not, create the table\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "        DeltaTable.createOrReplace(spark).tableName(\"github_data\").location(delta_table_path).execute()\n",
    "    else:\n",
    "        # If it exists, merge the new data\n",
    "        delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "        delta_table.alias(\"old_data\").merge(\n",
    "            df.alias(\"new_data\"),\n",
    "            \"old_data.id = new_data.id\"\n",
    "        ).whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d23ecef9-912d-4445-a31b-b12ba9944404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Demonstrate Benefits\n",
    "def demonstrate_benefits():\n",
    "    # Reliability: ACID transactions ensure data consistency\n",
    "    df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "    df.createOrReplaceTempView(\"github_data\")\n",
    "    \n",
    "    # Scalability: handle large volumes of data efficiently\n",
    "    total_count = spark.sql(\"SELECT COUNT(*) FROM github_data\").collect()[0][0]\n",
    "    print(f\"Total records: {total_count}\")\n",
    "    \n",
    "    # Schema Enforcement and Evolution: demonstrate adding a new column\n",
    "    df = df.withColumn(\"processed_at\", lit(datetime.now().isoformat()))\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "    \n",
    "    # Query Performance: optimized for fast data retrieval\n",
    "    recent_issues = spark.sql(\"SELECT * FROM github_data WHERE type = 'issue' AND created_at >= date_sub(current_date(), 7)\")\n",
    "    recent_issues.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ef83cc-4c04-4211-914a-bb36e9566a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    issues, prs = fetch_github_data()\n",
    "    df = transform_data(issues, prs)\n",
    "    load_data_to_delta(df)\n",
    "    demonstrate_benefits()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
