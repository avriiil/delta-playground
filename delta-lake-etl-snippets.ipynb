{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39b9ae1f-0c7a-425d-bf5e-f3bafc5aeed6",
   "metadata": {},
   "source": [
    "# Delta Lake for ETL: Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84b438bf-f481-47a1-89a8-7119e3f64748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by initializing a Spark session with Delta Lake\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.master(\"local[4]\").appName(\"parallel\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119e8ad-7a09-41cd-9b8a-2063df1e6913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77226c46-398b-4ebc-baf7-dd69db627f5a",
   "metadata": {},
   "source": [
    "## Query Performance: ELT vs ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7a6fe7-c140-4eed-b7e3-091530693fbc",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583cd1de-eb00-4d4c-82ef-7237c96a8d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1489aaf-210b-44e3-b0de-6c442601829e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7c6c35-3e01-4a7e-a7fb-b5b0d93f466b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a410bf84-04ba-4cd0-a9ce-d663cd6c57e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d898057-a86d-4adf-974b-f42fe2c80b9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e055754f-22f0-420e-af84-6408887bb830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455aaee3-5372-4cb3-92ab-3b53a9452920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5580346-4678-4ec9-827a-34a3ad64fbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# read in 8M rows\n",
    "df = spark.read.parquet(\"s3://avriiil/census-large.parquet\")\n",
    "\n",
    "# query relevant data\n",
    "df.where(df.income == \"=>50K\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3024ff11-3132-4120-bced-d10e3088e2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b909ba-b544-4c25-b998-8d0327aab86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 8M rows\n",
    "df2= spark.read.csv(\"data/census_16M.csv\", header=True)\n",
    "\n",
    "# load data to Delta table\n",
    "df2.write.format(\"delta\").partitionBy(\"income\").save(\"data/delta_census_part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef75d12-90be-4cdd-b9fb-95a17bd6c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# query data\n",
    "df4 = spark.read.format(\"delta\").load(\"data/delta_census_part\")\n",
    "df4.where(df4.income == \"=>50K\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f635bff-8db6-4e4f-877f-d254cc31b714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c8d4b-54e2-46a7-937c-d2bd9156a48f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cbda7638-7952-4143-86bd-4239ef999a72",
   "metadata": {},
   "source": [
    "### Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f39019e0-0529-4794-9ad5-a8fe14f76aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.8 ms, sys: 3.19 ms, total: 8.99 ms\n",
      "Wall time: 21.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# read in 8M rows\n",
    "df = spark.read.csv(\"data/census_16M.csv\", header=True)\n",
    "\n",
    "# query relevant data\n",
    "df.where(df.income == \"=>50K\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e292d3c8-8584-4fbb-97f4-418fece33194",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e0df88-4585-4f3e-a303-012dcbca8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 8M rows\n",
    "df2= spark.read.csv(\"data/census_16M.csv\", header=True)\n",
    "\n",
    "# load data to Delta table\n",
    "df2.write.format(\"delta\").partitionBy(\"income\").save(\"data/delta_census_part\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68598fe8-9fea-4e14-a2f2-5ef9678ef87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.07 ms, sys: 2.61 ms, total: 6.67 ms\n",
      "Wall time: 20.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# query data\n",
    "df4 = spark.read.format(\"delta\").load(\"data/delta_census_part\")\n",
    "df4.where(df4.income == \"=>50K\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a97f0-e084-4a70-8674-01a535281fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbbdd0ef-792a-49d3-aa09-8616026cc7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in 8M rows\n",
    "df= spark.read.csv(\"data/census_16M.csv\", header=True)\n",
    "\n",
    "# load data to Delta table\n",
    "df.write.format(\"delta\").partitionBy(\"income\").save(\"data/delta_census\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11b9a397-e858-4819-99f1-7c39b6e58bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: string, workclass: string, fnlwgt: string, education: string, education_num: string, marital_status: string, occupation: string, relationship: string, race: string, sex: string, capital_gain: string, capital_loss: string, hours_per_week: string, native_country: string, income: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in partitioned delta\n",
    "df2 = spark.read.format(\"delta\").load(\"data/delta_census\")\n",
    "\n",
    "# repartition overwrite\n",
    "df2.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"age\").option(\"overwriteSchema\", \"true\").save(\"data/delta_census\")\n",
    "\n",
    "# run query on new partition column\n",
    "df3 = spark.read.format(\"delta\").load(\"data/delta_census\")\n",
    "df3.where(df3.age == 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfde309a-8c93-487f-98e6-51e3038fb953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: string, workclass: string, fnlwgt: string, education: string, education_num: string, marital_status: string, occupation: string, relationship: string, race: string, sex: string, capital_gain: string, capital_loss: string, hours_per_week: string, native_country: string, income: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"data/delta_census\")\n",
    "\n",
    "# run query on old partition column\n",
    "df.where(df.income == \"=>50K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5ad816-e860-4cd1-91e8-678ef7d79988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6f3672-5d64-4bbb-b5b2-025c104d1b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf8a64-0dfa-4ac9-af8e-56eb836600de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54957e11-aa93-42e1-b48d-10fb4179b0c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddae6ee-1d46-41ed-8710-8a1ce8e5c8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa12a1-99e0-414c-a6af-097d2e5a443b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3016718-f79e-4c3c-8ff8-e83f5758ff99",
   "metadata": {},
   "source": [
    "## create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef1b8d3-d799-421e-a184-308ad842c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a1a3bf1-094e-4b18-a9b5-7939b10bc7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(\"data/census_16M.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6bb471b-5fdf-429a-9c09-c7ff3ba8a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.concat([ddf, ddf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7d4704d-c902-4599-9a91-457a806ccff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78d288b5-a905-411f-9c8c-cfe8db196159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mddf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3://avriiil/census-large.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pyspark-350-delta-320/lib/python3.11/site-packages/dask_expr/_collection.py:3266\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m   3263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_parquet\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3264\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask_expr\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 3266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pyspark-350-delta-320/lib/python3.11/site-packages/dask_expr/io/parquet.py:653\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, compression, write_index, append, overwrite, ignore_divisions, partition_on, storage_options, custom_metadata, write_metadata_file, compute, compute_kwargs, schema, name_function, filesystem, engine, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m     out \u001b[38;5;241m=\u001b[39m new_collection(\n\u001b[1;32m    633\u001b[0m         ToParquet(\n\u001b[1;32m    634\u001b[0m             df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n\u001b[1;32m    650\u001b[0m     )\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[0;32m--> 653\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;66;03m# Invalidate the filesystem listing cache for the output path after write.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;66;03m# We do this before returning, even if `compute=False`. This helps ensure\u001b[39;00m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# that reading files that were just written succeeds.\u001b[39;00m\n\u001b[1;32m    658\u001b[0m fs\u001b[38;5;241m.\u001b[39minvalidate_cache(path)\n",
      "File \u001b[0;32m~/miniforge3/envs/pyspark-350-delta-320/lib/python3.11/site-packages/dask_expr/_collection.py:476\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[0;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    475\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[0;32m--> 476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDaskMethodsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pyspark-350-delta-320/lib/python3.11/site-packages/dask/base.py:375\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniforge3/envs/pyspark-350-delta-320/lib/python3.11/site-packages/dask/base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniforge3/envs/pyspark-350-delta-320/lib/python3.11/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/pyspark-350-delta-320/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ddf.to_parquet(\"s3://avriiil/census-large.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c6b6f-d47b-4b5d-8cf7-807de5329e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.to_csv(\"s3://avriiil/census-large.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ae1fe4-3286-48a2-b235-5db65945e425",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "321282f0-05ab-443e-bd51-a3cedd48011f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9bf6204-4fbd-4bcb-bb59-b21e8206bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/census_2M.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be46d18f-3b63-449e-bf1b-30a1d0080220",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f57edd6c-d33e-4599-969d-7061904695dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d17697-d051-4e7a-a1bb-60760ffd4a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/census_4M.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3392c76f-c477-4d3e-b230-0609d85bf937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5868629f-480d-4662-a2e3-b34b00f2ec7b",
   "metadata": {},
   "source": [
    "## Schema Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2268f0f-9ef0-4261-9138-1906611b4276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"bob\", 47), (\"li\", 23), (\"leonard\", 51)]).toDF(\n",
    "    \"first_name\", \"age\"\n",
    ")\n",
    "\n",
    "df.write.format(\"delta\").save(\"data/toy_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eadeb86-01c1-4b5f-830d-d2c5bffaa4f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: e5484814-093f-4565-a22e-1d7124c3755e).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- first_name: string (nullable = true)\n-- age: long (nullable = true)\n\n\nData schema:\nroot\n-- first_name: string (nullable = true)\n-- age: long (nullable = true)\n-- country: string (nullable = true)\n\n         ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrank\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m68\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musa\u001b[39m\u001b[38;5;124m\"\u001b[39m), (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjordana\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m26\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrasil\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\u001b[38;5;241m.\u001b[39mtoDF(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m )\n\u001b[0;32m----> 5\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/toy_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pyspark-350-delta-320/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/pyspark-350-delta-320/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/pyspark-350-delta-320/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [_LEGACY_ERROR_TEMP_DELTA_0007] A schema mismatch detected when writing to the Delta table (Table ID: e5484814-093f-4565-a22e-1d7124c3755e).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- first_name: string (nullable = true)\n-- age: long (nullable = true)\n\n\nData schema:\nroot\n-- first_name: string (nullable = true)\n-- age: long (nullable = true)\n-- country: string (nullable = true)\n\n         "
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"frank\", 68, \"usa\"), (\"jordana\", 26, \"brasil\")]).toDF(\n",
    "    \"first_name\", \"age\", \"country\"\n",
    ")\n",
    "\n",
    "df.write.format(\"delta\").mode(\"append\").save(\"data/toy_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fe1640d-a6ac-4aa9-969c-a94e5bb22274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.option(\"mergeSchema\", \"true\").mode(\"append\").format(\"delta\").save(\n",
    "    \"data/toy_data\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dab8b77-9e2e-4b25-9a30-c16679d1139b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
